{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a689a8a7",
   "metadata": {},
   "source": [
    "#  COM-304. Reiforcement Learning Notebook\n",
    "\n",
    "The main goal of this notebook is to demonstrate how to use Habitat-Sim simulator and Habitat-Lab library for trianing agents. You will evaluate a pre-trained agent that uses visual observations for navigation and implement a Policy Gradient (REINFORCE) algorithm to train a simple blind agent that uses only GPS+Compass sensors for navigation. This notebook achieves best performance in training and evaluation with GPUs. However, it is possible to run it on CPU, but it will be much slower.\n",
    "\n",
    "There are two types of exercises in this notebook:\n",
    "1. Coding exercise -- you will need to implement some missing parts of the code. You will recognize them by `TODO:` comments.\n",
    "2. Questions -- you will need to analyse results and unswer an open-ended question in text . There are N of them. You will recognize them by `Question (X points)` title. **Be concise in your answers.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "521a48db",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "id": "2b2f4baf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T10:33:23.916196Z",
     "start_time": "2024-04-26T10:33:23.912603Z"
    }
   },
   "source": [
    "import seaborn as sns\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 100\n",
    "sns.set_style(\"whitegrid\")\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "ea7d6f6e",
   "metadata": {},
   "source": [
    "# Introduction to Habitat\n",
    "There are two important components of Habitat:\n",
    "1. [Habitat-Sim](https://github.com/facebookresearch/habitat-sim), a simulator with photo-realistic rendering and rigid-body mechanics,\n",
    "2. [Habitat-Lab](https://github.com/facebookresearch/habitat-lab), a modular high-level library for end-to-end development in embodied AI.\n",
    "\n",
    "There are also [challenges](https://aihabitat.org/challenge/2023/) in Habitat for various tasks, such as PointGoal Navigation, ObjectGoal Navigation, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b8fdb7e",
   "metadata": {},
   "source": [
    "# Build our first environment\n",
    "Let's first build our environment with Habitat-Sim. There are three important components in Habitat-Sim:\n",
    "1. multiple sensors\n",
    "2. configurable embodied agents\n",
    "3. scenes that can be loaded from various datasets, such as [Gibson](http://gibsonenv.stanford.edu/database/), [Matterport3D](https://niessner.github.io/Matterport/), [Habitat Matterport 3D](https://aihabitat.org/datasets/hm3d/), etc.\n",
    "\n",
    "Thus, to create an simulation environment, we need to specify the configuration for the sensors, agent, and simulator backend (including the scene)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5194fcf",
   "metadata": {},
   "source": [
    "## Imports\n",
    "We import some packages that will be used later."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T10:33:29.320437Z",
     "start_time": "2024-04-26T10:33:23.917437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import random\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import habitat_sim\n",
    "\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 100\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "sensor_settings = {\n",
    "    \"height\": 256, \"width\": 256,  # Spatial resolution of observations\n",
    "    \"sensor_height\": 1.5,  # Height of sensors in meters, relative to the agent\n",
    "}\n",
    "\n",
    "sim_settings = {\n",
    "    \"default_agent\": 0,  # Index of the default agent\n",
    "    \"scene_id\": \"data/scene_datasets/gibson/Cantwell.glb\",  # Scene file, episode 0 in val split of Gibson\n",
    "    \"enable_physics\": False,  # kinematics only\n",
    "    \"seed\": 42  # used in the random navigation\n",
    "}\n",
    "\n",
    "\n",
    "def main():\n",
    "    sensor_specs = config_sensors()\n",
    "    agent_cfg = agent_config(sensor_specs)\n",
    "    sim_cfg = sim_backend()\n",
    "    cfg = habitat_sim.Configuration(sim_cfg, [agent_cfg])\n",
    "    sim = habitat_sim.Simulator(cfg)\n",
    "\n",
    "    # Randomness is needed when choosing the actions\n",
    "    random.seed(sim_settings[\"seed\"])\n",
    "    sim.seed(sim_settings[\"seed\"])\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def config_sensors():\n",
    "    # Create a RGB sensor configuration\n",
    "    rgb_sensor_spec = habitat_sim.CameraSensorSpec()\n",
    "    rgb_sensor_spec.uuid = \"color_sensor\"\n",
    "    rgb_sensor_spec.sensor_type = habitat_sim.SensorType.COLOR\n",
    "    rgb_sensor_spec.resolution = [sensor_settings[\"height\"], sensor_settings[\"width\"]]\n",
    "    rgb_sensor_spec.position = [0.0, sensor_settings[\"sensor_height\"], 0.0]\n",
    "    rgb_sensor_spec.sensor_subtype = habitat_sim.SensorSubType.PINHOLE\n",
    "\n",
    "    # Create a depth sensor configuration\n",
    "    depth_sensor_spec = habitat_sim.CameraSensorSpec()\n",
    "    depth_sensor_spec.uuid = \"depth_sensor\"\n",
    "    depth_sensor_spec.sensor_type = habitat_sim.SensorType.DEPTH\n",
    "    depth_sensor_spec.resolution = [sensor_settings[\"height\"], sensor_settings[\"width\"]]\n",
    "    depth_sensor_spec.position = [0.0, sensor_settings[\"sensor_height\"], 0.0]\n",
    "    depth_sensor_spec.sensor_subtype = habitat_sim.SensorSubType.PINHOLE\n",
    "\n",
    "    return [rgb_sensor_spec, depth_sensor_spec]\n",
    "\n",
    "\n",
    "def agent_config(sensor_specs):\n",
    "    agent_settings = {\n",
    "        \"action_space\": {\n",
    "            \"move_forward\": 0.25, \"move_backward\": 0.25,  # Distance to cover in a move action in meters\n",
    "            \"turn_left\": 30.0, \"turn_right\": 30,  # Angles to cover in a turn action in degrees\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Create an agent configuration\n",
    "    agent_cfg = habitat_sim.agent.AgentConfiguration()\n",
    "    agent_cfg.action_space = {\n",
    "        k: habitat_sim.agent.ActionSpec(\n",
    "            k, habitat_sim.agent.ActuationSpec(amount=v)\n",
    "        ) for k, v in agent_settings[\"action_space\"].items()\n",
    "    }\n",
    "    agent_cfg.sensor_specifications = sensor_specs\n",
    "\n",
    "    return agent_cfg\n",
    "\n",
    "\n",
    "def sim_backend():\n",
    "    # Create a simulator backend configuration\n",
    "    sim_cfg = habitat_sim.SimulatorConfiguration()\n",
    "    sim_cfg.scene_id = sim_settings[\"scene_id\"]\n",
    "    sim_cfg.enable_physics = sim_settings[\"enable_physics\"]\n",
    "\n",
    "    return sim_cfg\n",
    "\n",
    "# A utility function for displaying observations\n",
    "def display_obs(rgb_obs: np.ndarray, depth_obs: np.ndarray):\n",
    "    img_arr, title_arr = [], []\n",
    "\n",
    "    rgb_img = Image.fromarray(rgb_obs, mode=\"RGBA\")\n",
    "    img_arr.append(rgb_img)\n",
    "    title_arr.append(\"rgb\")\n",
    "\n",
    "    depth_img = Image.fromarray((depth_obs / 10 * 255).astype(np.uint8), mode=\"L\")\n",
    "    img_arr.append(depth_img)\n",
    "    title_arr.append(\"depth\")\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, (img, title) in enumerate(zip(img_arr, title_arr)):\n",
    "        ax = plt.subplot(1, 2, i + 1)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(title)\n",
    "        plt.imshow(img)\n",
    "    plt.show(block=False)\n",
    "\n",
    "\n",
    "def set_agent_state(sim):\n",
    "    # Set agent state\n",
    "    agent = sim.initialize_agent(sim_settings[\"default_agent\"])  # Get our default agent\n",
    "    agent_state = habitat_sim.AgentState()\n",
    "    agent_state.position = np.array([-4.69643, 0.15825, -2.90618])  # Position in world coordinate\n",
    "    agent.set_state(agent_state)\n",
    "\n",
    "    # Get agent state\n",
    "    agent_state = agent.get_state()\n",
    "    print(f\"Agent state: position ({agent_state.position}), rotation ({agent_state.rotation})\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ],
   "id": "9d4c1ef2a73509d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renderer: NVIDIA GeForce RTX 3050 Ti Laptop GPU/PCIe/SSE2 by NVIDIA Corporation\n",
      "OpenGL version: 4.6.0 NVIDIA 550.54.15\n",
      "Using optional features:\n",
      "    GL_ARB_vertex_array_object\n",
      "    GL_ARB_separate_shader_objects\n",
      "    GL_ARB_robustness\n",
      "    GL_ARB_texture_storage\n",
      "    GL_ARB_texture_view\n",
      "    GL_ARB_framebuffer_no_attachments\n",
      "    GL_ARB_invalidate_subdata\n",
      "    GL_ARB_texture_storage_multisample\n",
      "    GL_ARB_multi_bind\n",
      "    GL_ARB_direct_state_access\n",
      "    GL_ARB_get_texture_sub_image\n",
      "    GL_ARB_texture_filter_anisotropic\n",
      "    GL_KHR_debug\n",
      "    GL_KHR_parallel_shader_compile\n",
      "    GL_NV_depth_buffer_float\n",
      "Using driver workarounds:\n",
      "    no-forward-compatible-core-context\n",
      "    nv-egl-incorrect-gl11-function-pointers\n",
      "    no-layout-qualifiers-on-old-glsl\n",
      "    nv-zero-context-profile-mask\n",
      "    nv-implementation-color-read-format-dsa-broken\n",
      "    nv-cubemap-inconsistent-compressed-image-size\n",
      "    nv-cubemap-broken-full-compressed-image-query\n",
      "    nv-compressed-block-size-in-bits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:33:26:251822]:[Warning]:[Metadata] SceneDatasetAttributes.cpp(107)::addNewSceneInstanceToDataset : Dataset : 'default' : Lighting Layout Attributes 'no_lights' specified in Scene Attributes but does not exist in dataset, so creating default.\n",
      "[12:33:26:252163]:[Warning]:[Scene] SemanticScene.h(328)::checkFileExists : ::loadSemanticSceneDescriptor: Filedata/scene_datasets/gibson/Cantwell.scndoes not exist.  Aborting load.\n",
      "[12:33:26:252175]:[Warning]:[Scene] SemanticScene.cpp(121)::loadSemanticSceneDescriptor : SSD File Naming Issue! Neither SemanticAttributes-provided name : `data/scene_datasets/gibson/Cantwell.scn` nor constructed filename : `data/scene_datasets/gibson/info_semantic.json` exist on disk.\n",
      "[12:33:26:252182]:[Error]:[Scene] SemanticScene.cpp(137)::loadSemanticSceneDescriptor : SSD Load Failure! File with SemanticAttributes-provided name `data/scene_datasets/gibson/Cantwell.scn` exists but failed to load.\n",
      "[12:33:29:313521]:[Warning]:[Sim] Simulator.cpp(594)::instanceStageForSceneAttributes : The active scene does not contain semantic annotations : activeSemanticSceneID_ = 0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "27b50e95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T10:33:29.383382Z",
     "start_time": "2024-04-26T10:33:29.322600Z"
    }
   },
   "source": [
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import habitat_sim"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b92b9c9",
   "metadata": {},
   "source": [
    "## Sensor configuration\n",
    "First, let's configure the sensors. Here we use one RGB sensor and one depth sensor. We can customize many properties about the sensors, such as spatial resolution, position, rotation, etc."
   ]
  },
  {
   "cell_type": "code",
   "id": "1a64e70c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T10:33:29.390058Z",
     "start_time": "2024-04-26T10:33:29.384746Z"
    }
   },
   "source": [
    "sensor_settings = {\n",
    "    \"height\": 256, \"width\": 256,  # Spatial resolution of observations\n",
    "    \"sensor_height\": 1.5,  # Height of sensors in meters, relative to the agent\n",
    "}\n",
    "\n",
    "# Create a RGB sensor configuration\n",
    "rgb_sensor_spec = habitat_sim.CameraSensorSpec()\n",
    "rgb_sensor_spec.uuid = \"color_sensor\"\n",
    "rgb_sensor_spec.sensor_type = habitat_sim.SensorType.COLOR\n",
    "rgb_sensor_spec.resolution = [sensor_settings[\"height\"], sensor_settings[\"width\"]]\n",
    "rgb_sensor_spec.position = [0.0, sensor_settings[\"sensor_height\"], 0.0]\n",
    "rgb_sensor_spec.sensor_subtype = habitat_sim.SensorSubType.PINHOLE\n",
    "\n",
    "# Create a depth sensor configuration\n",
    "depth_sensor_spec = habitat_sim.CameraSensorSpec()\n",
    "depth_sensor_spec.uuid = \"depth_sensor\"\n",
    "depth_sensor_spec.sensor_type = habitat_sim.SensorType.DEPTH\n",
    "depth_sensor_spec.resolution = [sensor_settings[\"height\"], sensor_settings[\"width\"]]\n",
    "depth_sensor_spec.position = [0.0, sensor_settings[\"sensor_height\"], 0.0]\n",
    "depth_sensor_spec.sensor_subtype = habitat_sim.SensorSubType.PINHOLE\n",
    "\n",
    "sensor_specs = [rgb_sensor_spec, depth_sensor_spec]"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66f95b01",
   "metadata": {},
   "source": [
    "***Comment on the depth sensor:** while we directly pass the depth infromation to the agent, it can still be (approximately) derrived from the RGB signal via binocular vision or various depth cues. Therefore, we still treat this signal as visual.* "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa4f30ad",
   "metadata": {},
   "source": [
    "## Agent configuration\n",
    "Next, we configure the agent. Here, we specify the action space of the agent, such as the distance to move and angle to turn.\n",
    "\n",
    "Note that the sensor configurations are contained in the agent configuration since the sensors are rigidly attached to the agent."
   ]
  },
  {
   "cell_type": "code",
   "id": "85d05a30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T10:33:29.402744Z",
     "start_time": "2024-04-26T10:33:29.391174Z"
    }
   },
   "source": [
    "agent_settings = {\n",
    "    \"action_space\": {\n",
    "        \"move_forward\": 0.25, \"move_backward\": 0.25,  # Distance to cover in a move action in meters\n",
    "        \"turn_left\": 30.0, \"turn_right\": 30,  # Angles to cover in a turn action in degrees\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create an agent configuration\n",
    "agent_cfg = habitat_sim.agent.AgentConfiguration()\n",
    "agent_cfg.action_space = {\n",
    "    k: habitat_sim.agent.ActionSpec(\n",
    "        k, habitat_sim.agent.ActuationSpec(amount=v)\n",
    "    ) for k, v in agent_settings[\"action_space\"].items()\n",
    "}\n",
    "agent_cfg.sensor_specifications = sensor_specs"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a26cf038",
   "metadata": {},
   "source": [
    "## Simulator backend configuration\n",
    "Finally, we configure the simulator backend, such as the scene we want to use and whether to enable the physics."
   ]
  },
  {
   "cell_type": "code",
   "id": "e62c17f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T10:33:29.414950Z",
     "start_time": "2024-04-26T10:33:29.404337Z"
    }
   },
   "source": [
    "sim_settings = {\n",
    "    \"default_agent\": 0,  # Index of the default agent\n",
    "    \"scene_id\": \"data/scene_datasets/gibson/Cantwell.glb\",  # Scene file, episode 0 in val split of Gibson\n",
    "    \"enable_physics\": False,  # kinematics only\n",
    "    \"seed\": 42  # used in the random navigation\n",
    "}\n",
    "\n",
    "# Create a simulator backend configuration\n",
    "sim_cfg = habitat_sim.SimulatorConfiguration()\n",
    "sim_cfg.scene_id = sim_settings[\"scene_id\"]\n",
    "sim_cfg.enable_physics = sim_settings[\"enable_physics\"]"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22797996",
   "metadata": {},
   "source": [
    "## Configuration for the simulator\n",
    "Let's collect our configurations for the sensors, agent, and simulator backend into a configuration that can be understood by the simulator."
   ]
  },
  {
   "cell_type": "code",
   "id": "ee592a79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T10:33:29.433183Z",
     "start_time": "2024-04-26T10:33:29.419171Z"
    }
   },
   "source": [
    "# Create a configuration for the simulator\n",
    "cfg = habitat_sim.Configuration(sim_cfg, [agent_cfg])"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73b4ecec",
   "metadata": {},
   "source": [
    "## Create a simulator instance\n",
    "We build a simulator based on the configuration"
   ]
  },
  {
   "cell_type": "code",
   "id": "b7f63c27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T10:33:36.126766Z",
     "start_time": "2024-04-26T10:33:29.434703Z"
    }
   },
   "source": [
    "try:\n",
    "    sim.close()\n",
    "except NameError:\n",
    "    pass\n",
    "sim = habitat_sim.Simulator(cfg)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renderer: NVIDIA GeForce RTX 3050 Ti Laptop GPU/PCIe/SSE2 by NVIDIA Corporation\n",
      "OpenGL version: 4.6.0 NVIDIA 550.54.15\n",
      "Using optional features:\n",
      "    GL_ARB_vertex_array_object\n",
      "    GL_ARB_separate_shader_objects\n",
      "    GL_ARB_robustness\n",
      "    GL_ARB_texture_storage\n",
      "    GL_ARB_texture_view\n",
      "    GL_ARB_framebuffer_no_attachments\n",
      "    GL_ARB_invalidate_subdata\n",
      "    GL_ARB_texture_storage_multisample\n",
      "    GL_ARB_multi_bind\n",
      "    GL_ARB_direct_state_access\n",
      "    GL_ARB_get_texture_sub_image\n",
      "    GL_ARB_texture_filter_anisotropic\n",
      "    GL_KHR_debug\n",
      "    GL_KHR_parallel_shader_compile\n",
      "    GL_NV_depth_buffer_float\n",
      "Using driver workarounds:\n",
      "    no-forward-compatible-core-context\n",
      "    nv-egl-incorrect-gl11-function-pointers\n",
      "    no-layout-qualifiers-on-old-glsl\n",
      "    nv-zero-context-profile-mask\n",
      "    nv-implementation-color-read-format-dsa-broken\n",
      "    nv-cubemap-inconsistent-compressed-image-size\n",
      "    nv-cubemap-broken-full-compressed-image-query\n",
      "    nv-compressed-block-size-in-bits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:33:29:448056]:[Warning]:[Metadata] SceneDatasetAttributes.cpp(107)::addNewSceneInstanceToDataset : Dataset : 'default' : Lighting Layout Attributes 'no_lights' specified in Scene Attributes but does not exist in dataset, so creating default.\n",
      "[12:33:29:448151]:[Warning]:[Scene] SemanticScene.h(328)::checkFileExists : ::loadSemanticSceneDescriptor: Filedata/scene_datasets/gibson/Cantwell.scndoes not exist.  Aborting load.\n",
      "[12:33:29:448159]:[Warning]:[Scene] SemanticScene.cpp(121)::loadSemanticSceneDescriptor : SSD File Naming Issue! Neither SemanticAttributes-provided name : `data/scene_datasets/gibson/Cantwell.scn` nor constructed filename : `data/scene_datasets/gibson/info_semantic.json` exist on disk.\n",
      "[12:33:29:448165]:[Error]:[Scene] SemanticScene.cpp(137)::loadSemanticSceneDescriptor : SSD Load Failure! File with SemanticAttributes-provided name `data/scene_datasets/gibson/Cantwell.scn` exists but failed to load.\n",
      "[12:33:36:109868]:[Warning]:[Sim] Simulator.cpp(594)::instanceStageForSceneAttributes : The active scene does not contain semantic annotations : activeSemanticSceneID_ = 0\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50645bfd",
   "metadata": {},
   "source": [
    "## Manual navigation\n",
    "Let's navigate the agent in the scene and check what it sees.\n",
    "\n",
    "We declare some utility functions for displaying the observations received by the agent."
   ]
  },
  {
   "cell_type": "code",
   "id": "17153554",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T10:33:36.166858Z",
     "start_time": "2024-04-26T10:33:36.137297Z"
    }
   },
   "source": [
    "# Randomness is needed when choosing the actions\n",
    "random.seed(sim_settings[\"seed\"])\n",
    "sim.seed(sim_settings[\"seed\"])\n",
    "\n",
    "# A utility function for displaying observations\n",
    "def display_obs(rgb_obs: np.ndarray, depth_obs: np.ndarray):\n",
    "    img_arr, title_arr = [], []\n",
    "    \n",
    "    rgb_img = Image.fromarray(rgb_obs, mode=\"RGBA\")\n",
    "    img_arr.append(rgb_img)\n",
    "    title_arr.append(\"rgb\")\n",
    "    \n",
    "    depth_img = Image.fromarray((depth_obs / 10 * 255).astype(np.uint8), mode=\"L\")\n",
    "    img_arr.append(depth_img)\n",
    "    title_arr.append(\"depth\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, (img, title) in enumerate(zip(img_arr, title_arr)):\n",
    "        ax = plt.subplot(1, 2, i + 1)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(title)\n",
    "        plt.imshow(img)\n",
    "    plt.show(block=False)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc4f49d1",
   "metadata": {},
   "source": [
    "We first put the agent into a position we like, say, `[-4.69643, 0.15825, -2.90618]`."
   ]
  },
  {
   "cell_type": "code",
   "id": "b68abc06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T10:33:36.200517Z",
     "start_time": "2024-04-26T10:33:36.173284Z"
    }
   },
   "source": [
    "# Set agent state\n",
    "agent = sim.initialize_agent(sim_settings[\"default_agent\"])  # Get our default agent\n",
    "agent_state = habitat_sim.AgentState()\n",
    "agent_state.position = np.array([-4.69643, 0.15825, -2.90618])  # Position in world coordinate\n",
    "agent.set_state(agent_state)\n",
    "\n",
    "# Get agent state\n",
    "agent_state = agent.get_state()\n",
    "print(f\"Agent state: position ({agent_state.position}), rotation ({agent_state.rotation})\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent state: position ([-4.69643  0.15825 -2.90618]), rotation (quaternion(1, 0, 0, 0))\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95843fec",
   "metadata": {},
   "source": [
    "You can control the agent to `move_forward`, `move_backward`, `turn_left`, or `turn_right` by `T`, `G`, `F`, or `H`, respectively. If you do not want to continue, just press `O`.\n",
    "\n",
    "__TODO: CODING EXERCISE BELOW (10pts)__"
   ]
  },
  {
   "cell_type": "code",
   "id": "85654baa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T10:33:39.280619Z",
     "start_time": "2024-04-26T10:33:36.208817Z"
    }
   },
   "source": [
    "key_bindings = {\n",
    "    'T': 'move_forward',\n",
    "    'G': 'move_backward',\n",
    "    'F': 'turn_left',\n",
    "    'H': 'turn_right',\n",
    "}\n",
    "while True:\n",
    "    key_pressed = input('Use TGFH to control the agent now: ')\n",
    "    key_pressed = key_pressed.upper()\n",
    "    if key_pressed == 'O':\n",
    "        print(\"Bye...\")\n",
    "        break\n",
    "    elif key_pressed in key_bindings.keys():\n",
    "        action = key_bindings[key_pressed]\n",
    "        print(f\"Choose to {action}\")\n",
    "        # TODO: Please enter your code here to replace ...\n",
    "        # HINT: You can refer to the doc of Env at https://aihabitat.org/docs/habitat-lab/habitat.core.env.Env.html\n",
    "        observations = sim.step(action)\n",
    "        rgb_obs = observations.get(\"color_sensor\")\n",
    "        depth_obs = observations.get(\"depth_sensor\")\n",
    "\n",
    "        display_obs(rgb_obs, depth_obs)\n",
    "    else:\n",
    "        print(f\"Invalid input {key_pressed}. Please only use TGFH!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bye...\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9da23bae",
   "metadata": {},
   "source": [
    "# Using a trained agent\n",
    "In this section, we will close the loop and use a trained agent to navigate in the scene.\n",
    "\n",
    "We are going to use an agent trained for [PointGoal Navigation](https://arxiv.org/abs/1807.06757). Basically, the agent is asked to *go to ($\\Delta x$, $\\Delta y$)* relative to its start position without a map. \n",
    "\n",
    "The agent is usually equipped with a RGB camera, a depth camera, and an idealized (❓) GPS+compass sensor. The GPS+compass sensor provides the distance and direction towards the target.\n",
    "\n",
    "The reward used for training the agent contains 3 components:\n",
    "1. slack reward\n",
    "    $$\n",
    "    r_t^\\text{slack}=-0.01\n",
    "    $$\n",
    "2. progressed distance to goal reward\n",
    "    $$\n",
    "    r_t^\\text{progressed distance}=r_{t-1}-r_{t}\n",
    "    $$\n",
    "    where $r_{t}$ is the geodesic distance to the goal (i.e., shortest path length to the goal) at time $t$.\n",
    "3. success reward\n",
    "    $$\n",
    "    r_t^\\text{success}=\n",
    "    \\begin{cases}\n",
    "    2.5,& \\text{if reach goal}\\\\\n",
    "    0,  & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "    $$\n",
    "\n",
    "Instead of Habitat-Sim, we will use Habitat-Lab which provides an interface of vectorized environments and end-to-end reinforcement learning algorithms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f32c4e93",
   "metadata": {},
   "source": [
    "## Imports\n",
    "As usual, we import some packages used afterwards."
   ]
  },
  {
   "cell_type": "code",
   "id": "8aac3527",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-04-26T10:33:40.266650Z",
     "start_time": "2024-04-26T10:33:39.282421Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "from typing import Dict, List\n",
    "\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from omegaconf.dictconfig import DictConfig\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "\n",
    "from habitat import VectorEnv\n",
    "from habitat.utils.visualizations.utils import overlay_frame\n",
    "from habitat.utils.visualizations.utils import observations_to_image\n",
    "from habitat_baselines.common.baseline_registry import baseline_registry\n",
    "from habitat_baselines.common.habitat_env_factory import HabitatVectorEnvFactory\n",
    "from habitat_baselines.config.default import get_config\n",
    "from habitat_baselines.utils.common import (\n",
    "    batch_obs,\n",
    "    inference_mode,\n",
    ")\n",
    "from habitat_baselines.rl.ppo import PPO\n",
    "\n",
    "os.environ['MAGNUM_LOG'] = 'quiet'\n",
    "os.environ['HABITAT_SIM_LOG'] = 'quiet'"
   ],
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'overlay_frame' from 'habitat.utils.visualizations.utils' (/home/david/Desktop/EPFL/comm_proj/comm-proj-habitat-lab/habitat-lab/habitat/utils/visualizations/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 16\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhabitat\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m VectorEnv\n\u001B[0;32m---> 16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhabitat\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvisualizations\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m overlay_frame\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhabitat\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvisualizations\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m observations_to_image\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhabitat_baselines\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcommon\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbaseline_registry\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m baseline_registry\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'overlay_frame' from 'habitat.utils.visualizations.utils' (/home/david/Desktop/EPFL/comm_proj/comm-proj-habitat-lab/habitat-lab/habitat/utils/visualizations/utils.py)"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3265c571",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Rather than specifying the configurations on-the-fly in Habitat-Sim, we can load the configurations from a `yaml` file in Habitat-Lab. Here we use the configuration file `ppo_pointnav.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "id": "d64c7963",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T10:33:40.269355Z",
     "start_time": "2024-04-26T10:33:40.269231Z"
    }
   },
   "source": [
    "# A function to build the evaluation config for the trained agent\n",
    "def build_pretrained_config(data_path: str):\n",
    "    config = get_config(\"pointnav/ppo_pointnav.yaml\")  # Extract config from yaml\n",
    "    # Change for evaluation\n",
    "    OmegaConf.set_readonly(config, False)\n",
    "    config.habitat_baselines.eval_ckpt_path_dir=\"./data/checkpoints/gibson.pth\"  # Choose checkpoint\n",
    "    config.habitat_baselines.num_updates = -1\n",
    "    config.habitat_baselines.num_environments = 1\n",
    "    config.habitat_baselines.verbose = False\n",
    "    config.habitat.dataset.data_path = data_path\n",
    "    OmegaConf.set_readonly(config, True)\n",
    "\n",
    "    return config\n",
    "\n",
    "sample_config = build_pretrained_config(\"data/datasets/pointnav/gibson/v1/val/val_cantwell.json.gz\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d78653b3",
   "metadata": {},
   "source": [
    "## Build environment\n",
    "Let's build a vectorized envrionment from the configuration."
   ]
  },
  {
   "cell_type": "code",
   "id": "fe55e4b8",
   "metadata": {},
   "source": [
    "# A function to build a vectorized environment\n",
    "def build_env(config: DictConfig, multiprocess=True):\n",
    "    if not multiprocess:\n",
    "        import os\n",
    "        os.environ['HABITAT_ENV_DEBUG'] = '1'\n",
    "    test = HabitatVectorEnvFactory()\n",
    "    return test.construct_envs(\n",
    "        config=config,\n",
    "        workers_ignore_signals=False,\n",
    "        enforce_scenes_greater_eq_environments=True,\n",
    "    )\n",
    "\n",
    "sample_env = build_env(config=sample_config)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da89e1d4",
   "metadata": {},
   "source": [
    "## Build actor-critic and agent\n",
    "Let's load the actor-critic and agent from the checkpoint specified.\n",
    "\n",
    "__TODO: CODING EXERCISE BELOW (5pts)__"
   ]
  },
  {
   "cell_type": "code",
   "id": "d9e52347",
   "metadata": {},
   "source": [
    "# A function to load the pretrained agent\n",
    "def build_agent(config: DictConfig, env: VectorEnv, device: torch.device):\n",
    "    ppo_cfg = config.habitat_baselines.rl.ppo  # Extract config for PPO\n",
    "    policy = baseline_registry.get_policy(\n",
    "        config.habitat_baselines.rl.policy.main_agent.name\n",
    "    )\n",
    "    # TODO: Please enter your code here to replace ...\n",
    "    # HINT: You can refer to the doc of VectorEnv at https://aihabitat.org/docs/habitat-lab/habitat.core.vector_env.VectorEnv.html\n",
    "    observation_space = env.observation_spaces[0]\n",
    "    policy_action_space = env.action_spaces[0]\n",
    "    orig_policy_action_space = env.orig_action_spaces[0]\n",
    "\n",
    "    actor_critic = policy.from_config(  # Build the actor-critic\n",
    "        config,\n",
    "        observation_space,\n",
    "        policy_action_space,\n",
    "        orig_action_space=orig_policy_action_space,\n",
    "    )\n",
    "    actor_critic.to(device)\n",
    "\n",
    "    agent = PPO.from_config(  # Build the PPO agent\n",
    "        actor_critic=actor_critic,\n",
    "        config=ppo_cfg,\n",
    "    )\n",
    "    \n",
    "    ckpt_dict = torch.load(config.habitat_baselines.eval_ckpt_path_dir, map_location=\"cpu\")  # Load the checkpoint\n",
    "    agent.load_state_dict(ckpt_dict[\"state_dict\"])\n",
    "\n",
    "    actor_critic.eval()\n",
    "    agent.eval()\n",
    "\n",
    "    return actor_critic, agent\n",
    "\n",
    "sample_device = torch.device(\"cpu\")\n",
    "sample_actor_critic, sample_agent = build_agent(config=sample_config, env=sample_env, device=sample_device)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ab04d5f",
   "metadata": {},
   "source": [
    "## Build variables\n",
    "Since the agent loaded is a recurrent policy, we need to build some auxiliary variables that will be taken as input by the policy."
   ]
  },
  {
   "cell_type": "code",
   "id": "a849008a",
   "metadata": {},
   "source": [
    "# A function to build auxiliary variables for the policy\n",
    "def build_variables(config: DictConfig, actor_critic: nn.Module, device: torch.device):\n",
    "    test_recurrent_hidden_states = torch.zeros(  # Hidden recurrent state\n",
    "        config.habitat_baselines.num_environments,\n",
    "        actor_critic.num_recurrent_layers, \n",
    "        config.habitat_baselines.rl.ppo.hidden_size, \n",
    "        device=device\n",
    "    )\n",
    "    prev_actions = torch.zeros(  # Previous action\n",
    "        config.habitat_baselines.num_environments,\n",
    "        1,\n",
    "        device=device,\n",
    "        dtype=torch.long,\n",
    "    )\n",
    "    not_done_masks = torch.zeros(\n",
    "        config.habitat_baselines.num_environments,\n",
    "        1,\n",
    "        device=device,\n",
    "        dtype=torch.bool,\n",
    "    )\n",
    "\n",
    "    return test_recurrent_hidden_states, prev_actions, not_done_masks\n",
    "\n",
    "sample_test_recurrent_hidden_states, sample_prev_actions, sample_not_done_masks = build_variables(\n",
    "    config=sample_config, \n",
    "    actor_critic=sample_actor_critic, \n",
    "    device=sample_device\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bedb1039",
   "metadata": {},
   "source": [
    "## Agent navigation\n",
    "We will used the trained policy for PointGoal navigation now. The agent takes RGBD image and GPS+compass measurement as input. It was trained by [Proximal Policy Optimization](https://arxiv.org/abs/1707.06347), a reinforcement learning algorithm.\n",
    "\n",
    "We can evaluate the agent with the following metrics:\n",
    "1. success: `bool`\n",
    "2. success weighted by path length (SPL): `float`\n",
    "    $$\n",
    "    SPL=S\\frac{L}{\\max(P, L)}\n",
    "    $$\n",
    "    where $S$ is 1 or 0 indicating success or not, $P$ is the length of the agent's path, and $L$ is the length of the shortest path from the start point to the goal point (not necessariy the straight line between the start point and the goal point)\n",
    "3. undiscounted return: `float`\n",
    "\n",
    "__TODO: CODING EXERCISE BELOW (15pts)__\n",
    "* __FIRST: 5pts__\n",
    "* __SECOND: 10pts__"
   ]
  },
  {
   "cell_type": "code",
   "id": "da434bdc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# A function to map observations to actions using the pretained agent\n",
    "def step_agent(actor_critic: nn.Module, batch: Dict[str, torch.Tensor], test_recurrent_hidden_states: torch.Tensor, prev_actions: torch.Tensor, not_done_masks: torch.Tensor):\n",
    "    # TODO: Please enter your code here to replace ...\n",
    "    # HINT: You can refer to the resnet policy at https://github.com/facebookresearch/habitat-lab/blob/v0.2.3/habitat-baselines/habitat_baselines/rl/ddppo/policy/resnet_policy.py\n",
    "    with inference_mode():\n",
    "        (\n",
    "            _,\n",
    "            actions,\n",
    "            _,\n",
    "            test_recurrent_hidden_states,\n",
    "        ) = actor_critic.act(\n",
    "            batch,  # Observations\n",
    "            test_recurrent_hidden_states,  # Recurrent hidden states\n",
    "            prev_actions,  # Previous actions\n",
    "            not_done_masks,  # Masks indicating whether episodes have ended\n",
    "            deterministic=False,  # Non-deterministic actions for exploration\n",
    "        )\n",
    "\n",
    "        prev_actions.copy_(actions)  # type: ignore\n",
    "\n",
    "    return actions, test_recurrent_hidden_states\n",
    "\n",
    "\n",
    "# A function to excecute simulation within the environment\n",
    "def step_env(env: VectorEnv, actions: torch.Tensor):\n",
    "    # TODO: Please enter your code here to replace ...\n",
    "    # HINT: You can refer to the doc of VectorEnv at https://aihabitat.org/docs/habitat-lab/habitat.core.vector_env.VectorEnv.html\n",
    "    step_data = actions.tolist()[0]\n",
    "    outputs = env.step(step_data)\n",
    "\n",
    "    observations, rewards_l, dones, infos = [\n",
    "        list(x) for x in zip(*outputs)\n",
    "    ]\n",
    "    \n",
    "    return observations, rewards_l, dones, infos\n",
    "\n",
    "\n",
    "# A utility function to post-process the results\n",
    "def post_process(observations: List[Dict], dones: List[bool], device: torch.device):\n",
    "    batch = batch_obs(  # type: ignore\n",
    "        observations,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    not_done_masks = torch.tensor(\n",
    "        [[not done] for done in dones],\n",
    "        dtype=torch.bool,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    return batch, not_done_masks\n",
    "\n",
    "\n",
    "# A utility function to collect rewards in an episode\n",
    "def collect_rewards(rewards_l: List[float], ep_rewards: List[float]):\n",
    "    rewards = torch.tensor(\n",
    "        rewards_l, dtype=torch.float, device=\"cpu\"\n",
    "    ).unsqueeze(1)\n",
    "    ep_rewards.append(rewards[0].item())\n",
    "\n",
    "\n",
    "# A utility function to collect frames in an episode\n",
    "def collect_frames(batch: Dict[str, torch.Tensor], infos: List[Dict], not_done_masks: torch.Tensor, ep_frames: List[np.ndarray]):\n",
    "    frame = observations_to_image(\n",
    "        {k: v[0] for k, v in batch.items()}, infos[0]\n",
    "    )\n",
    "    if not not_done_masks[0].item():\n",
    "        # The last frame corresponds to the first frame of the next episode\n",
    "        # but the info is correct. So we use a black frame\n",
    "        frame = observations_to_image(\n",
    "            {k: v[0] * 0.0 for k, v in batch.items()}, infos[0]\n",
    "        )\n",
    "    frame = overlay_frame(frame, infos[0])\n",
    "    ep_frames.append(frame)\n",
    "\n",
    "\n",
    "# A utility function to generate video from collected frames\n",
    "def generate_video(ep_frames: List[np.ndarray], video_name: str, output_dir: str = \"output_video\", fps: int = 10):\n",
    "    output_dir = output_dir.lower().replace(\" \", \"_\")\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    video_name = video_name.lower().replace(\" \", \"_\")\n",
    "    \n",
    "    writer = imageio.get_writer(\n",
    "        os.path.join(output_dir, video_name),\n",
    "        fps=fps\n",
    "    )\n",
    "\n",
    "    frames_iter = tqdm.tqdm(ep_frames)\n",
    "    for fm in frames_iter:\n",
    "        writer.append_data(fm)\n",
    "    writer.close()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1631c44a",
   "metadata": {},
   "source": [
    "def loop_env(config: DictConfig, scene_name: str, agent_name: str, add_drift: bool = False, drift_func = None):\n",
    "    # Set the randomness requried later\n",
    "    random.seed(config.habitat.seed)\n",
    "    np.random.seed(config.habitat.seed)\n",
    "    torch.manual_seed(config.habitat.seed)\n",
    "\n",
    "    # Choose device for cuda or cpu\n",
    "    device = torch.device(\"cuda\", config.habitat_baselines.torch_gpu_id) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    # Build the vectorized environment\n",
    "    env = build_env(config=config)\n",
    "\n",
    "    # Build the actor-critic and agent from a checkpoint\n",
    "    actor_critic, agent = build_agent(config=config, env=env, device=device)\n",
    "\n",
    "    # Build auxiliary variables\n",
    "    test_recurrent_hidden_states, prev_actions, not_done_masks = build_variables(\n",
    "        config=config, \n",
    "        actor_critic=actor_critic, \n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    observations = env.reset()  # Reset the environment, e.g., move the agent back to its start location\n",
    "    batch = batch_obs(observations, device=device)\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "    sign, scale = None, None\n",
    "\n",
    "    n_step = 0\n",
    "    episodes_stats = {}\n",
    "    ep_rewards, ep_frames = [], []\n",
    "    while len(episodes_stats) < env.number_of_episodes[0]:\n",
    "        ep_name = env.current_episodes()[0].episode_id\n",
    "        actions, test_recurrent_hidden_states = step_agent(  # Map observations to actions\n",
    "            actor_critic=actor_critic,\n",
    "            batch=batch,\n",
    "            test_recurrent_hidden_states=test_recurrent_hidden_states,\n",
    "            prev_actions=prev_actions,\n",
    "            not_done_masks=not_done_masks,\n",
    "        )\n",
    "\n",
    "        observations, rewards_l, dones, infos = step_env(  # One step forward of simulation in the environment\n",
    "            env=env,\n",
    "            actions=actions,\n",
    "        )\n",
    "        if add_drift:\n",
    "            if sign is None:\n",
    "                sign = rng.choice((-1, 1))  # Direction for drift\n",
    "                scale = 0.05  # Scale the drift w.r.t. steps\n",
    "            drift_func(\n",
    "                observations=observations,\n",
    "                n_step=n_step,\n",
    "                sign=sign,\n",
    "                scale=scale\n",
    "            )\n",
    "\n",
    "        batch, not_done_masks = post_process(  # Post-process the results\n",
    "            observations=observations,\n",
    "            dones=dones,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        collect_rewards(  # Collect rewards\n",
    "            rewards_l=rewards_l,\n",
    "            ep_rewards=ep_rewards,\n",
    "        )\n",
    "\n",
    "        collect_frames(  # Collect frames\n",
    "            batch=batch,\n",
    "            infos=infos,\n",
    "            not_done_masks=not_done_masks,\n",
    "            ep_frames=ep_frames\n",
    "        )\n",
    "\n",
    "        n_step += 1\n",
    "        if not not_done_masks[0].item():  # Episode ended\n",
    "            last_infos = infos.copy()\n",
    "\n",
    "            episodes_stats[ep_name] = {\n",
    "                'success': last_infos[0]['success'],\n",
    "                'spl': last_infos[0]['spl'],\n",
    "                'return': sum(ep_rewards),\n",
    "            }\n",
    "\n",
    "            # Generate video\n",
    "            generate_video(\n",
    "                ep_frames=ep_frames,\n",
    "                video_name=f\"ep={ep_name}_success={last_infos[0]['success']}_spl={last_infos[0]['spl']}.mp4\",\n",
    "                output_dir=f\"output_video/{scene_name}/{agent_name}\"\n",
    "            )\n",
    "\n",
    "            # Clean\n",
    "            n_step = 0\n",
    "            ep_rewards, ep_frames = [], []\n",
    "\n",
    "            # Build auxiliary variables\n",
    "            test_recurrent_hidden_states, prev_actions, not_done_masks = build_variables(\n",
    "                config=config, \n",
    "                actor_critic=actor_critic, \n",
    "                device=device\n",
    "            )\n",
    "\n",
    "    success_l, spl_l, return_l = [], [], []\n",
    "    for ep_stat in episodes_stats.values():\n",
    "        success_l.append(ep_stat['success'])\n",
    "        spl_l.append(ep_stat['spl'])\n",
    "        return_l.append(ep_stat['return'])\n",
    "\n",
    "    avg_success = sum(success_l) / len(success_l)\n",
    "    avg_spl = sum(spl_l) / len(spl_l)\n",
    "    avg_return = sum(return_l) / len(return_l)\n",
    "\n",
    "    print(f\"In {scene_name}, {agent_name}\")\n",
    "    print(f\"\\t Average success rate: {avg_success}\")\n",
    "    print(f\"\\t Average SPL: {avg_spl}\")\n",
    "    print(f\"\\t Average return: {avg_return}\")\n",
    "\n",
    "    return {\n",
    "        'success': avg_success,\n",
    "        'spl': avg_spl,\n",
    "        'return': avg_return\n",
    "    }\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "16e61afc",
   "metadata": {},
   "source": [
    "# Build configuration\n",
    "config = build_pretrained_config(\"data/datasets/pointnav/gibson/v1/val/val_cantwell.json.gz\")\n",
    "\n",
    "# Start loop env for evaluation\n",
    "stats_pretrained_cantwell = loop_env(config=config, scene_name='Cantwell', agent_name='Pretrained Agent')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4689b30b",
   "metadata": {},
   "source": [
    "## Drift on GPS and compass measurements\n",
    "Did you feel something wrong with the **idealized** GPS+compass sensor (i.e., producing noiseless measurements for the distance and direction towards the target)? The GPS and compass measurements are usually drifted in indoor scenes. \n",
    "\n",
    "Let's add some simulated drift to GPS and compass measurements and see what will happen."
   ]
  },
  {
   "cell_type": "code",
   "id": "bf4b9f2f",
   "metadata": {},
   "source": [
    "# A functionn to simulate drift on the GPS and compass measurements\n",
    "# It adds drift which is a linear function to number of steps\n",
    "def simulate_drift(observations: List[Dict], n_step: int, sign: int, scale: float):\n",
    "    rho_drift = sign * n_step * scale\n",
    "    phi_drift = sign * n_step * np.pi * scale\n",
    "    observations[0]['pointgoal_with_gps_compass'][0] += rho_drift\n",
    "    observations[0]['pointgoal_with_gps_compass'][1] += phi_drift\n",
    "    observations[0]['pointgoal_with_gps_compass'][1] %= 2 * np.pi  # mod\n",
    "    observations[0]['pointgoal_with_gps_compass'][1] -= np.pi  # map to -pi~pi"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "866e1893",
   "metadata": {},
   "source": [
    "# Build the config using a new scene\n",
    "config = build_pretrained_config(\"data/datasets/pointnav/gibson/v1/val/val_cantwell.json.gz\")\n",
    "\n",
    "# Start loop env for evaluation\n",
    "stats_pretrained_drift_cantwell = loop_env(config=config, scene_name='Cantwell', agent_name='Pretrained Agent With Drift', add_drift=True, drift_func=simulate_drift)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "faa46b07",
   "metadata": {},
   "source": [
    "### **Question** (20 pts)\n",
    "\n",
    "1. What happens if we add drift to the GPS and compass measurements? *(observe)*\n",
    "2. Why do you think the agent does not rely more on vision and fails in this case? *(hypothesize)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a94315",
   "metadata": {},
   "source": [
    "__Answer:__\n",
    "1) The agent has a 0% success rate and all of the trials fail\n",
    "2) It has been trained using an idealized GPS+compass so the agent heavily relied on this data to get a good accuracy, and could have for example used the depth and RGB images to avoid walls and other obstacles instead of using them to get to the target. This result could also be due to insufficient training or lack of robustness in extracting and utilizing visual features from the environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "535da11b",
   "metadata": {},
   "source": [
    "# Control Baselines\n",
    "We observed how an agent trained for PointGoal navigation performs. Does it perform well or not? To answer this quesion, we need to put its performance into a context and introduce the following control baselines:\n",
    "1. Random walk -- provides the lower bound on the performance\n",
    "2. Shortest path -- provides the upper bound on the performance\n",
    "\n",
    "We will evaluate the baselines in two scenes: one is [Cantwell](http://gibsonenv.stanford.edu/models/?id=Cantwell) as used before while the other one is [Edgemere](http://gibsonenv.stanford.edu/models/?id=Edgemere). The topdown map of the two scenes are shown below.\n",
    "\n",
    "Cantwell             |  Edgemere\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"images/top_down_map_Cantwell.png\" alt=\"Topdown map of Cantwell\" width=\"50%\" height=\"50%\"/>  |  <img src=\"images/top_down_map_Edgemere.png\" alt=\"Topdown map of Edgemere\" width=\"50%\" height=\"50%\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3adf248d",
   "metadata": {},
   "source": [
    "## Vision-based agent\n",
    "Here we evalute the performance of our pretrained agent mentioned above in Edgemere."
   ]
  },
  {
   "cell_type": "code",
   "id": "41a807b8",
   "metadata": {},
   "source": [
    "# Build configuration\n",
    "config = build_pretrained_config(\"data/datasets/pointnav/gibson/v1/val/val_edgemere.json.gz\")\n",
    "\n",
    "# Start loop env for evaluation\n",
    "stats_pretrained_edgemere = loop_env(config=config, scene_name='Edgemere', agent_name='Pretrained Agent')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7f5bacd",
   "metadata": {},
   "source": [
    "## Random walk\n",
    "The agent randomly walks in the scene. Here, it randomly chooses the action among `move_forward`(1), `turn_left`(2), and `turn_right`(3). \n",
    "\n",
    "If set `include_stop=True`, the agent also includes `stop`(0) in to choices, which will cause the episode end very soon. Just have a try!"
   ]
  },
  {
   "cell_type": "code",
   "id": "cfbc5334",
   "metadata": {},
   "source": [
    "# A function to randomly choose an action\n",
    "def step_random_walk_agent(include_stop=True):\n",
    "    lower = 0 if include_stop else 1\n",
    "    return torch.randint(lower, 4, size=(1, 1), dtype=torch.long)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "317cbe7d",
   "metadata": {},
   "source": [
    "def loop_random_walk_env(config: DictConfig, scene_name: str, agent_name: str):\n",
    "    # Set the randomness requried later\n",
    "    random.seed(config.habitat.seed)\n",
    "    np.random.seed(config.habitat.seed)\n",
    "    torch.manual_seed(config.habitat.seed)\n",
    "\n",
    "    # Choose device for cuda or cpu\n",
    "    device = torch.device(\"cuda\", config.habitat_baselines.torch_gpu_id) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    # Build the vectorized environment\n",
    "    env = build_env(config=config)\n",
    "\n",
    "    observations = env.reset()  # Reset the environment\n",
    "    batch = batch_obs(observations, device=device)\n",
    "\n",
    "    n_step = 0\n",
    "    episodes_stats = {}\n",
    "    ep_rewards, ep_frames = [], []\n",
    "    last_info = None\n",
    "    while len(episodes_stats) < env.number_of_episodes[0]:\n",
    "        ep_name = env.current_episodes()[0].episode_id\n",
    "        actions = step_random_walk_agent()  # Randomly choose an action\n",
    "\n",
    "        observations, rewards_l, dones, infos = step_env(  # One step forward of simulation in the environment\n",
    "            env=env,\n",
    "            actions=actions,\n",
    "        )\n",
    "\n",
    "        batch, not_done_masks = post_process(  # Post-process the results\n",
    "            observations=observations,\n",
    "            dones=dones,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        collect_rewards(  # Collect rewards\n",
    "            rewards_l=rewards_l,\n",
    "            ep_rewards=ep_rewards,\n",
    "        )\n",
    "\n",
    "        collect_frames(  # Collect frames\n",
    "            batch=batch,\n",
    "            infos=infos,\n",
    "            not_done_masks=not_done_masks,\n",
    "            ep_frames=ep_frames\n",
    "        )\n",
    "\n",
    "        n_step += 1\n",
    "        if not not_done_masks[0].item():  # Episode ended\n",
    "            last_infos = infos.copy()\n",
    "\n",
    "            episodes_stats[ep_name] = {\n",
    "                'success': last_infos[0]['success'],\n",
    "                'spl': last_infos[0]['spl'],\n",
    "                'return': sum(ep_rewards),\n",
    "            }\n",
    "\n",
    "            # Generate video\n",
    "            generate_video(\n",
    "                ep_frames=ep_frames,\n",
    "                video_name=f\"ep={ep_name}_success={last_infos[0]['success']}_spl={last_infos[0]['spl']}.mp4\",\n",
    "                output_dir=f\"output_video/{scene_name}/{agent_name}\"\n",
    "            )\n",
    "\n",
    "            # Clean\n",
    "            n_step = 0\n",
    "            ep_rewards, ep_frames = [], []\n",
    "\n",
    "    success_l, spl_l, return_l = [], [], []\n",
    "    for ep_stat in episodes_stats.values():\n",
    "        success_l.append(ep_stat['success'])\n",
    "        spl_l.append(ep_stat['spl'])\n",
    "        return_l.append(ep_stat['return'])\n",
    "\n",
    "    avg_success = sum(success_l) / len(success_l)\n",
    "    avg_spl = sum(spl_l) / len(spl_l)\n",
    "    avg_return = sum(return_l) / len(return_l)\n",
    "\n",
    "    print(f\"In {scene_name}, {agent_name}\")\n",
    "    print(f\"\\t Average success rate: {avg_success}\")\n",
    "    print(f\"\\t Average SPL: {avg_spl}\")\n",
    "    print(f\"\\t Average return: {avg_return}\")\n",
    "\n",
    "    return {\n",
    "        'success': avg_success,\n",
    "        'spl': avg_spl,\n",
    "        'return': avg_return\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43c0a2b6",
   "metadata": {},
   "source": [
    "### Evaluation in Cantwell"
   ]
  },
  {
   "cell_type": "code",
   "id": "e43d2610",
   "metadata": {},
   "source": [
    "# Build configuration\n",
    "config = build_pretrained_config(\"data/datasets/pointnav/gibson/v1/val/val_cantwell.json.gz\")\n",
    "\n",
    "# Start loop random walk env for evaluation\n",
    "stats_random_walk_cantwell = loop_random_walk_env(config=config, scene_name='Cantwell', agent_name='Random Walk Agent')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b643aa2d",
   "metadata": {},
   "source": [
    "### Evaluation in Edgemere"
   ]
  },
  {
   "cell_type": "code",
   "id": "73e1d30d",
   "metadata": {},
   "source": [
    "# Build configuration\n",
    "config = build_pretrained_config(\"data/datasets/pointnav/gibson/v1/val/val_edgemere.json.gz\")\n",
    "\n",
    "# Start loop random walk env for evaluation\n",
    "stats_random_walk_edgemere = loop_random_walk_env(config=config, scene_name='Edgemere', agent_name='Random Walk Agent')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "547ecc28",
   "metadata": {},
   "source": [
    "## Shortest path\n",
    "This agent knows the ground truth of map and follows the shortest path towards the target position.\n",
    "\n",
    "We can compare its performance against our agent trained with RL. Let's see which one uses fewer steps to arrive at the target position."
   ]
  },
  {
   "cell_type": "code",
   "id": "1d810488",
   "metadata": {},
   "source": [
    "from habitat.tasks.nav.shortest_path_follower import ShortestPathFollower\n",
    "\n",
    "\n",
    "# A function to build the agent that knows the shortest path towards the target\n",
    "def build_shortest_path_agent(env: VectorEnv):\n",
    "    # unwrap env\n",
    "    internal_env = env.call_at(0, function_name='env').env.env._env._env\n",
    "    sim = internal_env.sim\n",
    "\n",
    "    follower = ShortestPathFollower(  # Build the shortest path follower\n",
    "        sim=sim,\n",
    "        goal_radius=0.1,\n",
    "        return_one_hot=False,\n",
    "    )\n",
    "\n",
    "    return follower, internal_env\n",
    "\n",
    "\n",
    "# A function to choose the best action from the shortest path follower\n",
    "def step_shortest_path_agent(follower: ShortestPathFollower, goal_pos: List[float]):\n",
    "    best_action = follower.get_next_action(goal_pos=goal_pos)\n",
    "\n",
    "    return torch.tensor([[best_action]], dtype=torch.long)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b1744e22",
   "metadata": {},
   "source": [
    "def loop_shortest_path_env(config: DictConfig, scene_name: str, agent_name: str):\n",
    "    # Set the randomness requried later\n",
    "    random.seed(config.habitat.seed)\n",
    "    np.random.seed(config.habitat.seed)\n",
    "    torch.manual_seed(config.habitat.seed)\n",
    "\n",
    "    # Choose device for cuda or cpu\n",
    "    device = torch.device(\"cuda\", config.habitat_baselines.torch_gpu_id) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    # Build the vectorized environment without multi-processing\n",
    "    env = build_env(config=config, multiprocess=False)\n",
    "    agent, internal_env = build_shortest_path_agent(env=env)\n",
    "\n",
    "    observations = env.reset()  # Reset the environment\n",
    "    batch = batch_obs(observations, device=device)\n",
    "\n",
    "    n_step = 0\n",
    "    episodes_stats = {}\n",
    "    ep_rewards, ep_frames = [], []\n",
    "    while len(episodes_stats) < env.number_of_episodes[0]:\n",
    "        ep_name = env.current_episodes()[0].episode_id\n",
    "        actions = step_shortest_path_agent(agent, goal_pos=internal_env.current_episode.goals[0].position)  # Choose the optimal action towards the target\n",
    "\n",
    "        observations, rewards_l, dones, infos = step_env(  # One step forward of simulation in the environment\n",
    "            env=env,\n",
    "            actions=actions,\n",
    "        )\n",
    "\n",
    "        batch, not_done_masks = post_process(  # Post-process the results\n",
    "            observations=observations,\n",
    "            dones=dones,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        collect_rewards(  # Collect rewards\n",
    "            rewards_l=rewards_l,\n",
    "            ep_rewards=ep_rewards,\n",
    "        )\n",
    "\n",
    "        collect_frames(  # Collect frames\n",
    "            batch=batch,\n",
    "            infos=infos,\n",
    "            not_done_masks=not_done_masks,\n",
    "            ep_frames=ep_frames\n",
    "        )\n",
    "\n",
    "        n_step += 1\n",
    "        if not not_done_masks[0].item():  # Episode ended\n",
    "            last_infos = infos.copy()\n",
    "\n",
    "            episodes_stats[ep_name] = {\n",
    "                'success': last_infos[0]['success'],\n",
    "                'spl': last_infos[0]['spl'],\n",
    "                'return': sum(ep_rewards),\n",
    "            }\n",
    "\n",
    "            # Generate video\n",
    "            generate_video(\n",
    "                ep_frames=ep_frames,\n",
    "                video_name=f\"ep={ep_name}_success={last_infos[0]['success']}_spl={last_infos[0]['spl']}.mp4\",\n",
    "                output_dir=f\"output_video/{scene_name}/{agent_name}\"\n",
    "            )\n",
    "\n",
    "            # Clean\n",
    "            n_step = 0\n",
    "            ep_rewards, ep_frames = [], []\n",
    "\n",
    "\n",
    "    success_l, spl_l, return_l = [], [], []\n",
    "    for ep_stat in episodes_stats.values():\n",
    "        success_l.append(ep_stat['success'])\n",
    "        spl_l.append(ep_stat['spl'])\n",
    "        return_l.append(ep_stat['return'])\n",
    "\n",
    "    avg_success = sum(success_l) / len(success_l)\n",
    "    avg_spl = sum(spl_l) / len(spl_l)\n",
    "    avg_return = sum(return_l) / len(return_l)\n",
    "\n",
    "    print(f\"In {scene_name}, {agent_name}\")\n",
    "    print(f\"\\t Average success rate: {avg_success}\")\n",
    "    print(f\"\\t Average SPL: {avg_spl}\")\n",
    "    print(f\"\\t Average return: {avg_return}\")\n",
    "\n",
    "    return {\n",
    "        'success': avg_success,\n",
    "        'spl': avg_spl,\n",
    "        'return': avg_return\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3ca080b",
   "metadata": {},
   "source": [
    "### Evaluation in Cantwell"
   ]
  },
  {
   "cell_type": "code",
   "id": "67e44228",
   "metadata": {},
   "source": [
    "# Build configuration\n",
    "config = build_pretrained_config(\"data/datasets/pointnav/gibson/v1/val/val_cantwell.json.gz\")\n",
    "\n",
    "# Start loop shortest path env for evaluation\n",
    "stats_shortest_path_cantwell = loop_shortest_path_env(config=config, scene_name='Cantwell', agent_name='Shortest Path Agent')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4e21577",
   "metadata": {},
   "source": [
    "### Evaluation in Edgemere"
   ]
  },
  {
   "cell_type": "code",
   "id": "0f715be8",
   "metadata": {},
   "source": [
    "# Build configuration\n",
    "config = build_pretrained_config(\"data/datasets/pointnav/gibson/v1/val/val_edgemere.json.gz\")\n",
    "\n",
    "# Start loop shortest path env for evaluation\n",
    "stats_shortest_path_edgemere = loop_shortest_path_env(config=config, scene_name='Edgemere', agent_name='Shortest Path Agent')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce47e217",
   "metadata": {},
   "source": [
    "## Blind (GPS+Compass only) Agent\n",
    "Our previous trained agent takes both RGBD images and GPS+compass measurements as input.\n",
    "As we observed before (introducing the drift to the scensors), the agent relies a lot on the GPS+compass measurements.\n",
    "In this section, we will estimate *whether vision is useful for navigation*.\n",
    "For that we load an agent that only uses GPS+compass measurements for navigation."
   ]
  },
  {
   "cell_type": "code",
   "id": "03747a84",
   "metadata": {},
   "source": [
    "# A function to build the evaludation config for the GPS+compass only agent\n",
    "def build_state_only_config(data_path: str):\n",
    "    config = get_config(\"pointnav/ppo_pointnav.yaml\")\n",
    "    # Minor change\n",
    "    OmegaConf.set_readonly(config, False)\n",
    "    config.habitat_baselines.eval_ckpt_path_dir = \"data/checkpoints/gibson_blind.pth\"  # Choose blind checkpoint\n",
    "    config.habitat_baselines.num_updates = -1\n",
    "    config.habitat_baselines.num_environments = 1\n",
    "    config.habitat_baselines.verbose = False\n",
    "    config.habitat_baselines.force_blind_policy = True\n",
    "    config.habitat.dataset.data_path = data_path\n",
    "    OmegaConf.set_readonly(config, True)\n",
    "\n",
    "    return config\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f288b855",
   "metadata": {},
   "source": [
    "### Evaluation in Cantwell"
   ]
  },
  {
   "cell_type": "code",
   "id": "5715a669",
   "metadata": {},
   "source": [
    "# Build config for state only agent\n",
    "config = build_state_only_config(\"data/datasets/pointnav/gibson/v1/val/val_cantwell.json.gz\")\n",
    "\n",
    "# Start loop env for evaluation\n",
    "stats_blind_cantwell = loop_env(config=config, scene_name='Cantwell', agent_name='Pretrained Agent without Vision')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7f504d7",
   "metadata": {},
   "source": [
    "### Evaluation in Edgemere"
   ]
  },
  {
   "cell_type": "code",
   "id": "c2835f22",
   "metadata": {},
   "source": [
    "# Build config for state only agent\n",
    "config = build_state_only_config(\"data/datasets/pointnav/gibson/v1/val/val_edgemere.json.gz\")\n",
    "\n",
    "# Start loop env for evaluation\n",
    "stats_blind_edgemere = loop_env(config=config, scene_name='Edgemere', agent_name='Pretrained Agent without Vision')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f51f96fc",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=['scene', 'agent', 'success', 'spl', 'return'])\n",
    "df = pd.concat([\n",
    "    df, \n",
    "    pd.DataFrame({\n",
    "        'scene': 'Cantwell',\n",
    "        'agent': 'Shortest Path',\n",
    "        **stats_shortest_path_cantwell,\n",
    "    }, index=[0]),\n",
    "    pd.DataFrame({\n",
    "        'scene': 'Edgemere',\n",
    "        'agent': 'Shortest Path',\n",
    "        **stats_shortest_path_edgemere,\n",
    "    }, index=[0]),\n",
    "    pd.DataFrame({\n",
    "        'scene': 'Cantwell',\n",
    "        'agent': 'Blind Agent',\n",
    "        **stats_blind_cantwell,\n",
    "    }, index=[0]),\n",
    "    pd.DataFrame({\n",
    "        'scene': 'Edgemere',\n",
    "        'agent': 'Blind Agent',\n",
    "        **stats_blind_edgemere,\n",
    "    }, index=[0]),\n",
    "    pd.DataFrame({\n",
    "        'scene': 'Cantwell',\n",
    "        'agent': 'Vision Agent',\n",
    "        **stats_pretrained_cantwell,\n",
    "    }, index=[0]),\n",
    "    pd.DataFrame({\n",
    "        'scene': 'Edgemere',\n",
    "        'agent': 'Vision Agent',\n",
    "        **stats_pretrained_edgemere,\n",
    "    }, index=[0]),\n",
    "    pd.DataFrame({\n",
    "        'scene': 'Cantwell',\n",
    "        'agent': 'Random Walk',\n",
    "        **stats_random_walk_cantwell,\n",
    "    }, index=[0]),\n",
    "    pd.DataFrame({\n",
    "        'scene': 'Edgemere',\n",
    "        'agent': 'Random Walk',\n",
    "        **stats_random_walk_edgemere,\n",
    "    }, index=[0]),\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c56684d5",
   "metadata": {},
   "source": [
    "sns.barplot(x='scene', y='success', hue='agent', data=df)\n",
    "plt.title('Success')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a95fbb03",
   "metadata": {},
   "source": [
    "sns.barplot(x='scene', y='spl', hue='agent', data=df)\n",
    "plt.title('SPL')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "965aa7e5",
   "metadata": {},
   "source": [
    "### **Question** (30 pts)\n",
    "\n",
    "1. Is there a difference in the performance of the blind agent and agent with addiitonal visual signal? How does it vary w.r.t. the scene? *(observe)* \n",
    "2. What do you think is the reason? *(hypothesize)*\n",
    "3. How would you answer the question \"Is vision useful for navigation\"? *(conclude)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f233199",
   "metadata": {},
   "source": [
    "__Answer:__\n",
    "1) The agent with a visual input consistantly performs better than the blind agent accross both in success rate and SPL. However, when considering a relatively simple map like Edgemere where there aren't many walls, the success rate is similiar because the agent doesn't have to go around walls, but the SPL is still superior for the seeing one. On the other hand in the Cantwell scene the visual signal greatly helps the agent to navigate effectively which is reflected in a much better score accross the board.\n",
    "2) The reason is probably that the agent uses vision to avoid obstacles that wouldn't be visible otherwise using GPS. For example it allows it to avoid walls, furniture, etc... where the blind agent would encounter a lot more aliasing in it's policy and unsurmountable obstacles.\n",
    "3) Yes, vision is important for navigation as it greatly improves the performance of the agent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8d31aa8",
   "metadata": {},
   "source": [
    "# Train a state-based policy\n",
    "In this section, we will train the GPS+compass agent in a simple room. We will compare two reinforcement learning algorithms:\n",
    "1. Policy Gradient (REINFORCE)\n",
    "2. Proximal Policy Optimization (PPO)\n",
    "\n",
    "*You will get to implement the REINFORCE algorithm* in this section, and use PPO from the Habitat-Lab library."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02c38ebe",
   "metadata": {},
   "source": [
    "## Imports\n",
    "We now import some packages used later."
   ]
  },
  {
   "cell_type": "code",
   "id": "ae4393ec",
   "metadata": {},
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from habitat_baselines.config.default import get_config\n",
    "from habitat_baselines.rl.ppo.ppo_trainer import PPOTrainer\n",
    "\n",
    "from pg.base_pg import BasePolicyGradient\n",
    "from pg.base_pg_trainer import BasePolicyGradientTrainer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "226c1164",
   "metadata": {},
   "source": [
    "## REINFORCE\n",
    "REINFORCE is an on-policy reinforcement learning algorithm. The update process could be desribe as follow:\n",
    "1. Initialize the policy parameters $\\theta$\n",
    "2. Collect rollouts with policy $\\pi_\\theta$: $(s_0, a_0, r_0, s_1, a_1, \\cdots, s_T)$\n",
    "3. For $t=T, T-1, \\cdots, 1$,\n",
    "    1. Calculate the return $R_t$\n",
    "    2. Update the policy parameters $\\theta=\\theta+\\alpha R_t\\nabla_\\theta \\log(\\pi_\\theta(a_t|s_t))$, where $\\alpha$ is a learning rate.\n",
    "\n",
    "Ready? Let's implement the loss computation of REINFORCE below. \n",
    "\n",
    "Note that we do not expect REINFORCE to perform well in the PointGoal Navigation task. So please just implement the action loss and value loss computation as shown above. No further tuning is required.\n",
    "\n",
    "__TODO: CODING EXERCISE BELOW (20pts)__\n",
    "* __FIRST: 10pts__\n",
    "* __SECOND: 10pts__"
   ]
  },
  {
   "cell_type": "code",
   "id": "624bb36f",
   "metadata": {},
   "source": [
    "class REINFORCE(BasePolicyGradient):\n",
    "    def _compute_action_loss(\n",
    "            self,\n",
    "            advantages: torch.Tensor,  # Advantage, A_t\n",
    "            returns: torch.Tensor,  # Return, R_t\n",
    "            action_log_probs: torch.Tensor,  # Log action probability, log(pi(a_t|s_t))\n",
    "            values: torch.Tensor  # Value prediction, V_t\n",
    "    ):\n",
    "        # Assuming pytorch will optimize using gradients\n",
    "        action_loss = -torch.mean(returns * action_log_probs)\n",
    "\n",
    "        return action_loss\n",
    "\n",
    "    def _compute_value_loss(\n",
    "            self,\n",
    "            advantages: torch.Tensor,  # Advantage, A_t\n",
    "            returns: torch.Tensor,  # Return, R_t\n",
    "            action_log_probs: torch.Tensor,  # Log action probability, log(pi(a_t|s_t))\n",
    "            values: torch.Tensor  # Value prediction, V_t\n",
    "    ):\n",
    "        # TODO: Please enter your code here to replace ...\n",
    "        value_loss = F.mse_loss(returns, values)\n",
    "\n",
    "        return value_loss\n",
    "\n",
    "class REINFORCETrainer(BasePolicyGradientTrainer):\n",
    "    def _agent_type(self):\n",
    "        return REINFORCE"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "05291d71",
   "metadata": {},
   "source": [
    "# A function to build configuration for REINFORCE training\n",
    "def build_REINFORCE_config():\n",
    "    config = get_config(\"pointnav/ppo_pointnav.yaml\")\n",
    "    # Change for REINFORCE\n",
    "    OmegaConf.set_readonly(config, False)\n",
    "    config.habitat_baselines.checkpoint_folder = \"data/REINFORCE_checkpoints\"\n",
    "    config.habitat_baselines.tensorboard_dir = \"tb/REINFORCE\"\n",
    "    config.habitat_baselines.num_updates = -1\n",
    "    config.habitat_baselines.num_environments = 2\n",
    "    config.habitat_baselines.verbose = False\n",
    "    config.habitat_baselines.num_checkpoints = -1\n",
    "    config.habitat_baselines.checkpoint_interval = 1000000\n",
    "    config.habitat_baselines.total_num_steps = 150 * 1000\n",
    "    config.habitat_baselines.force_blind_policy = True\n",
    "    config.habitat_baselines.rl.ppo.use_gae = False  # Use Monte-Carlo estimation for returns\n",
    "    config.habitat.dataset.data_path=\"data/datasets/pointnav/simple_room/v0/{split}/empty_room.json.gz\"\n",
    "    OmegaConf.set_readonly(config, True)\n",
    "\n",
    "    return config\n",
    "\n",
    "config = build_REINFORCE_config()\n",
    "\n",
    "# Set randomness\n",
    "random.seed(config.habitat.seed)\n",
    "np.random.seed(config.habitat.seed)\n",
    "torch.manual_seed(config.habitat.seed)\n",
    "if (\n",
    "    config.habitat_baselines.force_torch_single_threaded\n",
    "    and torch.cuda.is_available()\n",
    "):\n",
    "    torch.set_num_threads(1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "69da4542",
   "metadata": {},
   "source": [
    "import os\n",
    "os.environ[\"MAGNUM_LOG\"] = \"quiet\"\n",
    "os.environ[\"HABITAT_SIM_LOG\"] = \"quiet\"\n",
    "\n",
    "# Build the trainer and start training\n",
    "trainer = REINFORCETrainer(config)\n",
    "trainer.train()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0c6be81",
   "metadata": {},
   "source": [
    "## PPO\n",
    "Let us now train a PPO agent. We will use the PPO agent from Habitat-Lab."
   ]
  },
  {
   "cell_type": "code",
   "id": "0d2e34a3",
   "metadata": {},
   "source": [
    "# A function to build configuration for PPO training\n",
    "def build_PPO_config():\n",
    "    config = get_config(\"pointnav/ppo_pointnav.yaml\")\n",
    "    # Change for REINFORCE\n",
    "    OmegaConf.set_readonly(config, False)\n",
    "    config.habitat_baselines.checkpoint_folder = \"data/PPO_checkpoints\"\n",
    "    config.habitat_baselines.tensorboard_dir = \"tb/PPO\"\n",
    "    config.habitat_baselines.num_updates = -1\n",
    "    config.habitat_baselines.num_environments = 2\n",
    "    config.habitat_baselines.verbose = False\n",
    "    config.habitat_baselines.num_checkpoints = -1\n",
    "    config.habitat_baselines.checkpoint_interval = 1000000\n",
    "    config.habitat_baselines.total_num_steps = 150 * 1000\n",
    "    config.habitat_baselines.force_blind_policy = True\n",
    "    config.habitat.dataset.data_path=\"data/datasets/pointnav/simple_room/v0/{split}/empty_room.json.gz\"\n",
    "    OmegaConf.set_readonly(config, True)\n",
    "\n",
    "    return config\n",
    "\n",
    "config = build_PPO_config()  # Build the config for PPO\n",
    "\n",
    "# Set randomness\n",
    "random.seed(config.habitat.seed)\n",
    "np.random.seed(config.habitat.seed)\n",
    "torch.manual_seed(config.habitat.seed)\n",
    "if (\n",
    "    config.habitat_baselines.force_torch_single_threaded\n",
    "    and torch.cuda.is_available()\n",
    "):\n",
    "    torch.set_num_threads(1)\n",
    "\n",
    "import os\n",
    "os.environ[\"MAGNUM_LOG\"] = \"quiet\"\n",
    "os.environ[\"HABITAT_SIM_LOG\"] = \"quiet\"\n",
    "\n",
    "# Build the trainer and start training\n",
    "trainer = PPOTrainer(config)\n",
    "trainer.train()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "292d8fb1",
   "metadata": {},
   "source": [
    "## Comparing the performance of REINFORCE and PPO"
   ]
  },
  {
   "cell_type": "code",
   "id": "d5fc1269",
   "metadata": {},
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def parse_tensorboard(path, scalars, prefix=\"\"):\n",
    "    \"\"\"returns a dictionary of pandas dataframes for each requested scalar\"\"\"\n",
    "    ea = event_accumulator.EventAccumulator(\n",
    "        path,\n",
    "        size_guidance={event_accumulator.SCALARS: 0},\n",
    "    )\n",
    "    _ = ea.Reload()\n",
    "    df = pd.DataFrame(ea.Scalars(scalars[0]))\n",
    "    df[f'{scalars[0]}'] = df['value']\n",
    "    df.drop(columns=['value'], inplace=True)\n",
    "    for k in scalars:\n",
    "        df[k] = pd.DataFrame(ea.Scalars(k))['value']\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2d093c3d",
   "metadata": {},
   "source": [
    "# Parse the tensorboard logs\n",
    "reinforce_logs = parse_tensorboard(\"tb/REINFORCE\", [\"metrics/spl\", \"reward\", \"learner/grad_norm\"])\n",
    "ppo_logs = parse_tensorboard(\"tb/PPO\", [\"metrics/spl\", \"reward\", \"learner/grad_norm\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0ca9610c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(6, 6))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.axes(axes[0])\n",
    "plt.plot(reinforce_logs['metrics/spl'], label=\"REINFORCE\")\n",
    "plt.plot(ppo_logs['metrics/spl'], label=\"PPO\")\n",
    "plt.title(\"SPL\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.axes(axes[1])\n",
    "plt.plot(reinforce_logs['learner/grad_norm'], label=\"REINFORCE\")\n",
    "plt.plot(ppo_logs['learner/grad_norm'], label=\"PPO\")\n",
    "plt.title(\"Gradient Norm\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.legend()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65ce14ac",
   "metadata": {},
   "source": [
    "### **Question** (10 pts)\n",
    "1. What is the difference in the performance of REINFORCE and PPO? *(observe)*\n",
    "2. What do you think is the reasons for the difference? *(hypothesize)*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3787bd3e",
   "metadata": {},
   "source": [
    "__Answer:__\n",
    "1) PPO has better performance and reaches a better SPL than the REINFORCE algorithm (0.9 vs 0.45). Also the gradient in the REINFORCE fluctuates a lot and has trouble going down, while it's almost instantly 0 in the PPO case.\n",
    "2) REINFORCE suffers from a high variance in its policy updates as it relies on full episode rollouts to update the policy and does not explicitly reduce the variance of these updates. As we can see in the plot, the gradient norm fluctuates a lot indicating troubles of convergence. This causes slow convergence and sub-par performance.\n",
    "   On the other hand, PPO improves upon the stability and efficiency of policy updates. The clipping mechanism helps in reducing the variance of updates and prevents the policy from changing too drastically, which can destabilize learning. Therefore, it achieves faster convergence and much better performance as seen in the two graphs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
